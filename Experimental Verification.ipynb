{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653bef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install neccessary packages \n",
    "!pip install aif360==0.5.0\n",
    "!pip install fairlearn==0.10.0\n",
    "!pip install lime==0.2.0.1\n",
    "!pip install matplotlib==3.7.2\n",
    "!pip install numpy==1.24.3\n",
    "!pip install pandas==2.0.3\n",
    "!pip install plotly==5.9.0\n",
    "!pip install seaborn==0.12.2\n",
    "!pip install shap==0.44.1\n",
    "!pip install sklearn==1.3.0\n",
    "!pip install xgboost==1.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from aif360.sklearn.inprocessing import ExponentiatedGradientReduction\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# load dataset from SHRS_smallDS.xlsx\n",
    "df = pd.read_csv('Seer_Old datasets/bladder.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78528704",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_type_df = df[['age',\n",
    "                    ' behaviour_code',\n",
    "                    'gender_code',\n",
    "                    'cs_extension_code',\n",
    "                    'grade_code10',\n",
    "                    'histologic_type_code',\n",
    "                    'cs_lymph_nodes_code',\n",
    "                    'marital_status_code',\n",
    "                    'cs_mets_at_dx_code',\n",
    "                    'primary_site_code7',\n",
    "                    'race_code',\n",
    "                    ' radiation_code',\n",
    "                    ' number_of_nodes_examined',\n",
    "                    ' regional_positive_nodes ',\n",
    "                    'number_of_primaries',\n",
    "                    ' site_specific_surgery_code',\n",
    "                    'stage_of_cancer_code',\n",
    "                    'cs_tumor_size',\n",
    "                    'survived_code ']]\n",
    "\n",
    "cancer_type_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "cancer_type_df = cancer_type_df.dropna()\n",
    "\n",
    "cancer_type_df['survived_code '] = cancer_type_df['survived_code '].map({'yes': 1, 'no': 0})\n",
    "cancer_type_df['primary_site_code7'] = cancer_type_df['primary_site_code7'].str[1:]\n",
    "cancer_type_df['primary_site_code7'] = pd.to_numeric(cancer_type_df['primary_site_code7'], errors='raise')\n",
    "\n",
    "cancer_type_df['stage_of_cancer_code'] = cancer_type_df['stage_of_cancer_code'].replace(\" \", np.NAN)  # Replace with np.NAN or other value\n",
    "\n",
    "cancer_type_df = cancer_type_df.dropna(subset=['stage_of_cancer_code'])  # Drop rows with NaN in 'stage_of_cancer_code'\n",
    "\n",
    "# Try converting 'stage_of_cancer_code' to numeric, handling potential errors\n",
    "try:\n",
    "      cancer_type_df['stage_of_cancer_code'] = pd.to_numeric(cancer_type_df['stage_of_cancer_code'], errors='raise')\n",
    "except ValueError:\n",
    "      print(\"Error: Some values in 'stage_of_cancer_code' cannot be converted to numeric (after removing empty strings).\")\n",
    "\n",
    "cancer_type_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_type_df['primary_site_code7'] = pd.to_numeric(cancer_type_df['primary_site_code7'], errors='raise')\n",
    "cancer_type_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_type_stage_df = cancer_type_df[cancer_type_df['stage_of_cancer_code'] == 1]\n",
    "print(cancer_type_stage_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54edfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate array into input and output components\n",
    "X = cancer_type_stage_df.drop(['survived_code ', 'stage_of_cancer_code'], axis=1)\n",
    "y = cancer_type_stage_df['survived_code ']\n",
    "\n",
    "prot_attr_cols = [colname for colname in X \n",
    "                  if \"gender_code\" in colname or \"marital_status_code\" in colname \n",
    "                  or \"race_code\" in colname]\n",
    "prot_attr_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b92b9f",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015851e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "   \n",
    "# training the model on training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "   \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(gnb, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_estimator = GaussianNB()\n",
    "nb_exp_grad_red = ExponentiatedGradientReduction(prot_attr=prot_attr_cols, \n",
    "                                              estimator=nb_estimator, \n",
    "                                              constraints=\"EqualizedOdds\",\n",
    "                                              drop_prot_attr=False)\n",
    "nb_exp_grad_red.fit(X_train, y_train)\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(nb_exp_grad_red, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16883114",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cef836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# split the train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                       test_size=0.2)\n",
    "# LogisticRegression\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d814eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_estimator = LogisticRegression(solver='liblinear')\n",
    "lr_exp_grad_red = ExponentiatedGradientReduction(prot_attr=prot_attr_cols, \n",
    "                                              estimator=lr_estimator, \n",
    "                                              constraints=\"EqualizedOdds\",\n",
    "                                              drop_prot_attr=False)\n",
    "lr_exp_grad_red.fit(X_train, y_train)\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(lr_exp_grad_red, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_before = clf.predict(X_test)\n",
    "df_pred_before = X_test.copy()\n",
    "df_pred_before['survived_code'] = y_pred_before\n",
    "df_pred_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Convert the DataFrame to a BinaryLabelDataset\n",
    "predicted_dataset_before = BinaryLabelDataset(df=df_pred_before,\n",
    "                                       label_names=['survived_code'],\n",
    "                                       protected_attribute_names=prot_attr_cols,\n",
    "                                       favorable_label=1,  \n",
    "                                       unfavorable_label=0)  \n",
    "predicted_dataset_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_after = lr_exp_grad_red.predict(X_test)\n",
    "df_pred_after = X_test.copy()\n",
    "df_pred_after['survived_code'] = y_pred_after\n",
    "df_pred_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a BinaryLabelDataset\n",
    "predicted_dataset_after = BinaryLabelDataset(df=df_pred_after,\n",
    "                                       label_names=['survived_code'],\n",
    "                                       protected_attribute_names=prot_attr_cols,\n",
    "                                       favorable_label=1,  \n",
    "                                       unfavorable_label=0)  \n",
    "predicted_dataset_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a79d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = X_test.copy()\n",
    "true_dataset['survived_code'] = y_test\n",
    "true_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d3338",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = BinaryLabelDataset(df=true_dataset,\n",
    "                                   label_names=['survived_code'],\n",
    "                                   protected_attribute_names=prot_attr_cols,\n",
    "                                   favorable_label=1,  \n",
    "                                   unfavorable_label=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# For the model before fairness intervention\n",
    "metric_before = ClassificationMetric(true_dataset, predicted_dataset_before,\n",
    "                                    unprivileged_groups=[{'gender_code': 2}],\n",
    "                                    privileged_groups=[{'gender_code': 1}])\n",
    "\n",
    "tpr_before_unpriv = metric_before.true_positive_rate(privileged=False)\n",
    "fpr_before_unpriv = metric_before.false_positive_rate(privileged=False)\n",
    "\n",
    "tpr_before_priv = metric_before.true_positive_rate(privileged=True)\n",
    "fpr_before_priv = metric_before.false_positive_rate(privileged=True)\n",
    "\n",
    "# For the model after fairness intervention\n",
    "metric_after = ClassificationMetric(true_dataset, predicted_dataset_after,\n",
    "                                   unprivileged_groups=[{'gender_code': 2}],\n",
    "                                   privileged_groups=[{'gender_code': 1}])\n",
    "\n",
    "tpr_after_unpriv = metric_after.true_positive_rate(privileged=False)\n",
    "fpr_after_unpriv = metric_after.false_positive_rate(privileged=False)\n",
    "\n",
    "tpr_after_priv = metric_after.true_positive_rate(privileged=True)\n",
    "fpr_after_priv = metric_after.false_positive_rate(privileged=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the points for the before and after models\n",
    "plt.figure()\n",
    "# Plot for the model before fairness intervention\n",
    "plt.scatter(fpr_before_unpriv, tpr_before_unpriv, label='Unprivileged Group (Female) - Before', color='red')\n",
    "plt.scatter(fpr_before_priv, tpr_before_priv, label='Privileged Group (Male) - Before', color='blue')\n",
    "\n",
    "# Plot for the model after fairness intervention\n",
    "plt.scatter(fpr_after_unpriv, tpr_after_unpriv, label='Unprivileged Group (Female) - After', color='pink')\n",
    "plt.scatter(fpr_after_priv, tpr_after_priv, label='Privileged Group (Male) - After', color='lightblue')\n",
    "\n",
    "# Line of equality\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Line of Equality')\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Equalized odds plot for Logistic Regression Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85060506",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unprivileged Group - Before: ' + '(' + str(fpr_before_unpriv) + ', ' + str(tpr_before_unpriv) + ')')\n",
    "print('Unprivileged Group - After: ' + '(' + str(fpr_after_unpriv) + ', ' +  str(tpr_after_unpriv) + ')')\n",
    "print('Privileged Group - Before: ' + '(' + str(fpr_before_priv) + ', ' + str(tpr_before_priv) + ')')\n",
    "print('Privileged Group - After: ' + '(' + str(fpr_after_priv) + ', ' + str(tpr_after_priv) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1c6cf",
   "metadata": {},
   "source": [
    "## Decission Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# split the train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                       test_size=0.2)\n",
    "# Create a decision tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the decision tree on the data\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e84bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_estimator = DecisionTreeClassifier()\n",
    "dt_exp_grad_red = ExponentiatedGradientReduction(prot_attr=prot_attr_cols, \n",
    "                                              estimator=dt_estimator, \n",
    "                                              constraints=\"EqualizedOdds\",\n",
    "                                              drop_prot_attr=False)\n",
    "dt_exp_grad_red.fit(X_train, y_train)\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(dt_exp_grad_red, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0dbdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_before = clf.predict(X_test)\n",
    "df_pred_before = X_test.copy()\n",
    "df_pred_before['survived_code'] = y_pred_before\n",
    "df_pred_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Convert the DataFrame to a BinaryLabelDataset\n",
    "predicted_dataset_before = BinaryLabelDataset(df=df_pred_before,\n",
    "                                       label_names=['survived_code'],\n",
    "                                       protected_attribute_names=prot_attr_cols,\n",
    "                                       favorable_label=1,  \n",
    "                                       unfavorable_label=0)  \n",
    "predicted_dataset_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09442ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_after = dt_exp_grad_red.predict(X_test)\n",
    "df_pred_after = X_test.copy()\n",
    "df_pred_after['survived_code'] = y_pred_after\n",
    "df_pred_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b28a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a BinaryLabelDataset\n",
    "predicted_dataset_after = BinaryLabelDataset(df=df_pred_after,\n",
    "                                       label_names=['survived_code'],\n",
    "                                       protected_attribute_names=prot_attr_cols,\n",
    "                                       favorable_label=1,  \n",
    "                                       unfavorable_label=0)  \n",
    "predicted_dataset_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = X_test.copy()\n",
    "true_dataset['survived_code'] = y_test\n",
    "true_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e081fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = BinaryLabelDataset(df=true_dataset,\n",
    "                                   label_names=['survived_code'],\n",
    "                                   protected_attribute_names=prot_attr_cols,\n",
    "                                   favorable_label=1,  \n",
    "                                   unfavorable_label=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93eb330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# For the model before fairness intervention\n",
    "metric_before = ClassificationMetric(true_dataset, predicted_dataset_before,\n",
    "                                    unprivileged_groups=[{'gender_code': 2}],\n",
    "                                    privileged_groups=[{'gender_code': 1}])\n",
    "\n",
    "tpr_before_unpriv = metric_before.true_positive_rate(privileged=False)\n",
    "fpr_before_unpriv = metric_before.false_positive_rate(privileged=False)\n",
    "\n",
    "tpr_before_priv = metric_before.true_positive_rate(privileged=True)\n",
    "fpr_before_priv = metric_before.false_positive_rate(privileged=True)\n",
    "\n",
    "# For the model after fairness intervention\n",
    "metric_after = ClassificationMetric(true_dataset, predicted_dataset_after,\n",
    "                                   unprivileged_groups=[{'gender_code': 2}],\n",
    "                                   privileged_groups=[{'gender_code': 1}])\n",
    "\n",
    "tpr_after_unpriv = metric_after.true_positive_rate(privileged=False)\n",
    "fpr_after_unpriv = metric_after.false_positive_rate(privileged=False)\n",
    "\n",
    "tpr_after_priv = metric_after.true_positive_rate(privileged=True)\n",
    "fpr_after_priv = metric_after.false_positive_rate(privileged=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be823d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the points for the before and after models\n",
    "plt.figure()\n",
    "# Plot for the model before fairness intervention\n",
    "plt.scatter(fpr_before_unpriv, tpr_before_unpriv, label='Unprivileged Group - Before', color='red')\n",
    "plt.scatter(fpr_before_priv, tpr_before_priv, label='Privileged Group - Before', color='blue')\n",
    "\n",
    "# Plot for the model after fairness intervention\n",
    "plt.scatter(fpr_after_unpriv, tpr_after_unpriv, label='Unprivileged Group - After', color='pink')\n",
    "plt.scatter(fpr_after_priv, tpr_after_priv, label='Privileged Group - After', color='lightblue')\n",
    "\n",
    "# Line of equality\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Line of Equality')\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Equalized Odds Plot for Decision Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2800d",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ce493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(ada, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_estimator = AdaBoostClassifier()\n",
    "ada_exp_grad_red = ExponentiatedGradientReduction(prot_attr=prot_attr_cols, \n",
    "                                              estimator=ada_estimator, \n",
    "                                              constraints=\"EqualizedOdds\",\n",
    "                                              drop_prot_attr=False)\n",
    "ada_exp_grad_red.fit(X_train, y_train)\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(ada_exp_grad_red, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fbbac1",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(xgb, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_estimator = XGBClassifier()\n",
    "xgb_exp_grad_red = ExponentiatedGradientReduction(prot_attr=prot_attr_cols, \n",
    "                                              estimator=xgb_estimator, \n",
    "                                              constraints=\"EqualizedOdds\",\n",
    "                                              drop_prot_attr=False)\n",
    "xgb_exp_grad_red.fit(X_train, y_train)\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(xgb_exp_grad_red, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_before = xgb.predict(X_test)\n",
    "df_pred_before = X_test.copy()\n",
    "df_pred_before['survived_code'] = y_pred_before\n",
    "df_pred_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Convert the DataFrame to a BinaryLabelDataset\n",
    "predicted_dataset_before = BinaryLabelDataset(df=df_pred_before,\n",
    "                                       label_names=['survived_code'],\n",
    "                                       protected_attribute_names=prot_attr_cols,\n",
    "                                       favorable_label=1,  \n",
    "                                       unfavorable_label=0)  \n",
    "predicted_dataset_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_after = xgb_exp_grad_red.predict(X_test)\n",
    "df_pred_after = X_test.copy()\n",
    "df_pred_after['survived_code'] = y_pred_after\n",
    "df_pred_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be195d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a BinaryLabelDataset\n",
    "predicted_dataset_after = BinaryLabelDataset(df=df_pred_after,\n",
    "                                       label_names=['survived_code'],\n",
    "                                       protected_attribute_names=prot_attr_cols,\n",
    "                                       favorable_label=1,  \n",
    "                                       unfavorable_label=0)  \n",
    "predicted_dataset_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89694049",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = X_test.copy()\n",
    "true_dataset['survived_code'] = y_test\n",
    "true_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = BinaryLabelDataset(df=true_dataset,\n",
    "                                   label_names=['survived_code'],\n",
    "                                   protected_attribute_names=prot_attr_cols,\n",
    "                                   favorable_label=1,  \n",
    "                                   unfavorable_label=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92645f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# For the model before fairness intervention\n",
    "metric_before = ClassificationMetric(true_dataset, predicted_dataset_before,\n",
    "                                    unprivileged_groups=[{'gender_code': 2}],\n",
    "                                    privileged_groups=[{'gender_code': 1}])\n",
    "\n",
    "tpr_before_unpriv = metric_before.true_positive_rate(privileged=False)\n",
    "fpr_before_unpriv = metric_before.false_positive_rate(privileged=False)\n",
    "\n",
    "tpr_before_priv = metric_before.true_positive_rate(privileged=True)\n",
    "fpr_before_priv = metric_before.false_positive_rate(privileged=True)\n",
    "\n",
    "# For the model after fairness intervention\n",
    "metric_after = ClassificationMetric(true_dataset, predicted_dataset_after,\n",
    "                                   unprivileged_groups=[{'gender_code': 2}],\n",
    "                                   privileged_groups=[{'gender_code': 1}])\n",
    "\n",
    "tpr_after_unpriv = metric_after.true_positive_rate(privileged=False)\n",
    "fpr_after_unpriv = metric_after.false_positive_rate(privileged=False)\n",
    "\n",
    "tpr_after_priv = metric_after.true_positive_rate(privileged=True)\n",
    "fpr_after_priv = metric_after.false_positive_rate(privileged=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112761fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the points for the before and after models\n",
    "plt.figure()\n",
    "# Plot for the model before fairness intervention\n",
    "plt.scatter(fpr_before_unpriv, tpr_before_unpriv, label='Unprivileged Group - Before', color='red')\n",
    "plt.scatter(fpr_before_priv, tpr_before_priv, label='Privileged Group - Before', color='blue')\n",
    "\n",
    "# Plot for the model after fairness intervention\n",
    "plt.scatter(fpr_after_unpriv, tpr_after_unpriv, label='Unprivileged Group - After', color='pink')\n",
    "plt.scatter(fpr_after_priv, tpr_after_priv, label='Privileged Group - After', color='lightblue')\n",
    "\n",
    "# Line of equality\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Line of Equality')\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Equalized Odds Plot for Logistic Regression Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b7b18b",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3800e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_estimator = RandomForestClassifier()\n",
    "rf_exp_grad_red = ExponentiatedGradientReduction(prot_attr=prot_attr_cols, \n",
    "                                              estimator=rf_estimator, \n",
    "                                              constraints=\"EqualizedOdds\",\n",
    "                                              drop_prot_attr=False)\n",
    "rf_exp_grad_red.fit(X_train, y_train)\n",
    "\n",
    "# Perform 5-fold cross-validation with accuracy scoring\n",
    "scores = cross_val_score(rf_exp_grad_red, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy across folds\n",
    "print(\"Average Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_before = rf.predict(X_test)\n",
    "df_pred_before = X_test.copy()\n",
    "df_pred_before['survived_code'] = y_pred_before\n",
    "df_pred_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4821c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Convert the DataFrame to a BinaryLabelDataset\n",
    "predicted_dataset_before = BinaryLabelDataset(df=df_pred_before,\n",
    "                                       label_names=['survived_code'],\n",
    "                                       protected_attribute_names=prot_attr_cols,\n",
    "                                       favorable_label=1,  \n",
    "                                       unfavorable_label=0)  \n",
    "predicted_dataset_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_after = rf_exp_grad_red.predict(X_test)\n",
    "df_pred_after = X_test.copy()\n",
    "df_pred_after['survived_code'] = y_pred_after\n",
    "df_pred_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdc0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a BinaryLabelDataset\n",
    "predicted_dataset_after = BinaryLabelDataset(df=df_pred_after,\n",
    "                                       label_names=['survived_code'],\n",
    "                                       protected_attribute_names=prot_attr_cols,\n",
    "                                       favorable_label=1,  \n",
    "                                       unfavorable_label=0)  \n",
    "predicted_dataset_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f793481",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = X_test.copy()\n",
    "true_dataset['survived_code'] = y_test\n",
    "true_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae14a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dataset = BinaryLabelDataset(df=true_dataset,\n",
    "                                   label_names=['survived_code'],\n",
    "                                   protected_attribute_names=prot_attr_cols,\n",
    "                                   favorable_label=1,  \n",
    "                                   unfavorable_label=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbe2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# For the model before fairness intervention\n",
    "metric_before = ClassificationMetric(true_dataset, predicted_dataset_before,\n",
    "                                    unprivileged_groups=[{'gender_code': 2}],\n",
    "                                    privileged_groups=[{'gender_code': 1}])\n",
    "\n",
    "tpr_before_unpriv = metric_before.true_positive_rate(privileged=False)\n",
    "fpr_before_unpriv = metric_before.false_positive_rate(privileged=False)\n",
    "\n",
    "tpr_before_priv = metric_before.true_positive_rate(privileged=True)\n",
    "fpr_before_priv = metric_before.false_positive_rate(privileged=True)\n",
    "\n",
    "# For the model after fairness intervention\n",
    "metric_after = ClassificationMetric(true_dataset, predicted_dataset_after,\n",
    "                                   unprivileged_groups=[{'gender_code': 2}],\n",
    "                                   privileged_groups=[{'gender_code': 1}])\n",
    "\n",
    "tpr_after_unpriv = metric_after.true_positive_rate(privileged=False)\n",
    "fpr_after_unpriv = metric_after.false_positive_rate(privileged=False)\n",
    "\n",
    "tpr_after_priv = metric_after.true_positive_rate(privileged=True)\n",
    "fpr_after_priv = metric_after.false_positive_rate(privileged=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the points for the before and after models\n",
    "plt.figure()\n",
    "# Plot for the model before fairness intervention\n",
    "plt.scatter(fpr_before_unpriv, tpr_before_unpriv, label='Unprivileged Group - Before', color='red')\n",
    "plt.scatter(fpr_before_priv, tpr_before_priv, label='Privileged Group - Before', color='blue')\n",
    "\n",
    "# Plot for the model after fairness intervention\n",
    "plt.scatter(fpr_after_unpriv, tpr_after_unpriv, label='Unprivileged Group - After', color='pink')\n",
    "plt.scatter(fpr_after_priv, tpr_after_priv, label='Privileged Group - After', color='lightblue')\n",
    "\n",
    "# Line of equality\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Line of Equality')\n",
    "\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Equalized Odds Plot for Logistic Regression Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dd773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
